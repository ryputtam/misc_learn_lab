{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7539897,"sourceType":"datasetVersion","datasetId":4390452}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rputtam/classification-using-spark-mllib?scriptVersionId=222771026\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:04:27.21217Z","iopub.execute_input":"2025-02-16T03:04:27.212502Z","iopub.status.idle":"2025-02-16T03:04:28.229641Z","shell.execute_reply.started":"2025-02-16T03:04:27.21247Z","shell.execute_reply":"2025-02-16T03:04:28.228479Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/loan-status-prediction/loan_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#### test changes sync","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T03:59:43.458586Z","iopub.execute_input":"2025-01-21T03:59:43.459753Z","iopub.status.idle":"2025-01-21T03:59:43.464218Z","shell.execute_reply.started":"2025-01-21T03:59:43.459715Z","shell.execute_reply":"2025-01-21T03:59:43.462927Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The objective of this notebook is to predict loan approval status of members.\n#### **Prediction Algorithm**: Logistic Regression\n#### **Library**: spark MLib\n\nThe notebook adheres to the following steps.\n1. Installing and importing packages\n2. Load the data\n3. Data Exploration (EDA)\n4. Data Splitting\n5. Data Preprocessing\n6. Model Building\n7. Model Summary - coefficients and pValues\n8. Model Prediction\n9. Model Evaluation\n10. Saving model artifact\n11. Applying the model on testdata","metadata":{}},{"cell_type":"markdown","source":"## Install and import necessary packages","metadata":{}},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2025-02-16T03:04:36.665078Z","iopub.execute_input":"2025-02-16T03:04:36.665871Z","iopub.status.idle":"2025-02-16T03:05:21.230319Z","shell.execute_reply.started":"2025-02-16T03:04:36.665842Z","shell.execute_reply":"2025-02-16T03:05:21.229091Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849767 sha256=e54501124a84a6e938946dcdd66b64236223f9fbae658c62dbed8b8aba521d19\n  Stored in directory: /root/.cache/pip/wheels/d9/1c/98/31e395a42d1735d18d42124971ecbbade844b50bb9845b6f4a\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#Creating spark session\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('classification with MLlib').getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2025-02-16T03:05:25.584089Z","iopub.execute_input":"2025-02-16T03:05:25.58445Z","iopub.status.idle":"2025-02-16T03:05:31.178256Z","shell.execute_reply.started":"2025-02-16T03:05:25.584422Z","shell.execute_reply":"2025-02-16T03:05:31.177217Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/02/16 03:05:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Importing packages\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2025-02-16T03:05:38.040301Z","iopub.execute_input":"2025-02-16T03:05:38.040707Z","iopub.status.idle":"2025-02-16T03:05:38.705824Z","shell.execute_reply.started":"2025-02-16T03:05:38.040676Z","shell.execute_reply":"2025-02-16T03:05:38.70482Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"code","source":"df=spark.read.csv('/kaggle/input/loan-status-prediction/loan_data.csv',inferSchema=True,header=True)","metadata":{"execution":{"iopub.status.busy":"2025-02-16T03:05:47.168535Z","iopub.execute_input":"2025-02-16T03:05:47.168901Z","iopub.status.idle":"2025-02-16T03:05:54.306039Z","shell.execute_reply.started":"2025-02-16T03:05:47.168875Z","shell.execute_reply":"2025-02-16T03:05:54.304797Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2025-02-16T03:06:00.568632Z","iopub.execute_input":"2025-02-16T03:06:00.569309Z","iopub.status.idle":"2025-02-16T03:06:00.920879Z","shell.execute_reply.started":"2025-02-16T03:06:00.56928Z","shell.execute_reply":"2025-02-16T03:06:00.919856Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DataFrame[summary: string, Loan_ID: string, Gender: string, Married: string, Dependents: string, Education: string, Self_Employed: string, ApplicantIncome: string, CoapplicantIncome: string, LoanAmount: string, Loan_Amount_Term: string, Credit_History: string, Property_Area: string, Loan_Status: string]"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":" # Data Exploration","metadata":{}},{"cell_type":"markdown","source":"1. Dataframe size\n2. Get the field names and their data types, and, number of numeric and categorical features\n3. A glimpse of data\n4. Missing values count\n5. Target variable distribution\n6. Continuous variables distribution - normality check and boxplot for outliers\n7. Categorical variables distribution\n8. Predictors relation with target","metadata":{}},{"cell_type":"code","source":"print('Records:',df.count(),'\\nColumns:', len(df.columns))","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:30:21.494454Z","iopub.execute_input":"2025-02-14T23:30:21.495549Z","iopub.status.idle":"2025-02-14T23:30:22.370776Z","shell.execute_reply.started":"2025-02-14T23:30:21.495439Z","shell.execute_reply":"2025-02-14T23:30:22.36856Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:30:29.106059Z","iopub.execute_input":"2025-02-14T23:30:29.106457Z","iopub.status.idle":"2025-02-14T23:30:29.115392Z","shell.execute_reply.started":"2025-02-14T23:30:29.106427Z","shell.execute_reply":"2025-02-14T23:30:29.114077Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_data_types=['IntegerType','DoubleType','FloatType','LongType']\nstring_data_types=['StringType']\n\ncat_cols=0\nnum_cols=0\nother_cols=0\n\nnum_cols_list=[]\ncat_cols_list=[]\noth_cols_list=[]\n\nfor field in df.schema.fields:\n    if str(field.dataType)[:-2] in string_data_types:\n        cat_cols_list.append(field.name)\n        cat_cols+=1\n    elif str(field.dataType)[:-2] in numeric_data_types:\n        num_cols_list.append(field.name)\n        num_cols+=1\n    else:\n        oth_cols_list.append(field.name)\n        other_cols+1\nprint('Numeric cols:', num_cols,num_cols_list,'\\nCategorical Cols:', cat_cols, cat_cols_list,'\\nOther Cols:',other_cols,oth_cols_list)  ","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:30:32.341865Z","iopub.execute_input":"2025-02-14T23:30:32.342216Z","iopub.status.idle":"2025-02-14T23:30:32.350542Z","shell.execute_reply.started":"2025-02-14T23:30:32.342192Z","shell.execute_reply":"2025-02-14T23:30:32.349292Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df.show(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T21:56:55.034776Z","iopub.execute_input":"2024-07-13T21:56:55.035855Z","iopub.status.idle":"2024-07-13T21:56:55.046295Z","shell.execute_reply.started":"2024-07-13T21:56:55.035797Z","shell.execute_reply":"2024-07-13T21:56:55.044132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Glimpse","metadata":{}},{"cell_type":"code","source":"df.toPandas().head()\n# numeric variables such as income, loan amount, term are considered as strings, but should be converted to numeric data types which is done in data preprocessing step","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:30:36.692319Z","iopub.execute_input":"2025-02-14T23:30:36.693697Z","iopub.status.idle":"2025-02-14T23:30:37.292163Z","shell.execute_reply.started":"2025-02-14T23:30:36.693632Z","shell.execute_reply":"2025-02-14T23:30:37.290928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Missing values Summary","metadata":{}},{"cell_type":"code","source":"missing_values_dict={}\nfor c in df.columns:\n    missing_values_dict[c]=df.agg(F.count(F.when(F.isnan(c) | F.col(c).isNull(),c))).collect()[0][0]\n    \nmissing_values_dict","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:30:41.159223Z","iopub.execute_input":"2025-02-14T23:30:41.159665Z","iopub.status.idle":"2025-02-14T23:30:44.629673Z","shell.execute_reply.started":"2025-02-14T23:30:41.159635Z","shell.execute_reply":"2025-02-14T23:30:44.628618Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Target class distribution","metadata":{}},{"cell_type":"code","source":"# Creating target variable\ndf = df.withColumn('target', F.when(F.col('Loan_Status') == 'Y',1).otherwise(0))\nprint(df.select('target').dtypes)\ndf.agg(F.mean('target')).collect()[0][0]","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:30:44.641997Z","iopub.execute_input":"2025-02-14T23:30:44.642542Z","iopub.status.idle":"2025-02-14T23:30:44.98599Z","shell.execute_reply.started":"2025-02-14T23:30:44.642477Z","shell.execute_reply":"2025-02-14T23:30:44.984729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_count=df.count()\n# Get the target class distribution\ndf_prop = df.groupBy(['Loan_Status'])\\\n.agg(F.count('Loan_ID').alias('appl_ct'))\\\n.withColumn('prop',F.round(F.col('appl_ct')/total_count*100,2))\\\n\ndf_prop.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:30:51.010241Z","iopub.execute_input":"2025-02-14T23:30:51.010699Z","iopub.status.idle":"2025-02-14T23:30:52.035824Z","shell.execute_reply.started":"2025-02-14T23:30:51.010668Z","shell.execute_reply":"2025-02-14T23:30:52.029729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_target_stats=df_prop.toPandas()\ndf_target_stats.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:30:55.96935Z","iopub.execute_input":"2025-02-14T23:30:55.970071Z","iopub.status.idle":"2025-02-14T23:30:56.234306Z","shell.execute_reply.started":"2025-02-14T23:30:55.970039Z","shell.execute_reply":"2025-02-14T23:30:56.233308Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.catplot(data=df_target_stats,x='Loan_Status',y='appl_ct',kind='bar')\n\nplt.xlabel('Loan Status')\nplt.ylabel('Count')\nplt.title('Distribution of Loan Status')","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:30:58.981003Z","iopub.execute_input":"2025-02-14T23:30:58.981997Z","iopub.status.idle":"2025-02-14T23:30:59.279788Z","shell.execute_reply.started":"2025-02-14T23:30:58.981963Z","shell.execute_reply":"2025-02-14T23:30:59.278534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary Statistics","metadata":{}},{"cell_type":"code","source":"df.describe().toPandas().head()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:02.982488Z","iopub.execute_input":"2025-02-14T23:31:02.982902Z","iopub.status.idle":"2025-02-14T23:31:04.910548Z","shell.execute_reply.started":"2025-02-14T23:31:02.982872Z","shell.execute_reply":"2025-02-14T23:31:04.909603Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.toPandas().describe()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:16.020489Z","iopub.execute_input":"2025-02-14T23:31:16.020902Z","iopub.status.idle":"2025-02-14T23:31:16.232916Z","shell.execute_reply.started":"2025-02-14T23:31:16.020875Z","shell.execute_reply":"2025-02-14T23:31:16.231626Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.toPandas().describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:26.914274Z","iopub.execute_input":"2025-02-14T23:31:26.914692Z","iopub.status.idle":"2025-02-14T23:31:27.071608Z","shell.execute_reply.started":"2025-02-14T23:31:26.914664Z","shell.execute_reply":"2025-02-14T23:31:27.070441Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"string_fields = [col for col,dtype in df.dtypes if dtype =='string']\nstring_fields.remove('Loan_ID')\nfor string_field in string_fields:\n    df.select(string_field).distinct().show(5)","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:30.551933Z","iopub.execute_input":"2025-02-14T23:31:30.552323Z","iopub.status.idle":"2025-02-14T23:31:32.28929Z","shell.execute_reply.started":"2025-02-14T23:31:30.552295Z","shell.execute_reply":"2025-02-14T23:31:32.287355Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_p=df.toPandas()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:35.270087Z","iopub.execute_input":"2025-02-14T23:31:35.270559Z","iopub.status.idle":"2025-02-14T23:31:35.365558Z","shell.execute_reply.started":"2025-02-14T23:31:35.270487Z","shell.execute_reply":"2025-02-14T23:31:35.361777Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the levels of caterogical variables\nfor i in string_fields:\n    print(i,(df_p[i].nunique(),df_p[i].unique()))\n#df_p.Gender.value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:37.352244Z","iopub.execute_input":"2025-02-14T23:31:37.353304Z","iopub.status.idle":"2025-02-14T23:31:37.362649Z","shell.execute_reply.started":"2025-02-14T23:31:37.353267Z","shell.execute_reply":"2025-02-14T23:31:37.361105Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data distribution plots of continuous variables","metadata":{}},{"cell_type":"code","source":"num_cols_list","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:42.125143Z","iopub.execute_input":"2025-02-14T23:31:42.125477Z","iopub.status.idle":"2025-02-14T23:31:42.133108Z","shell.execute_reply.started":"2025-02-14T23:31:42.125455Z","shell.execute_reply":"2025-02-14T23:31:42.13164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n\nfig,axs = plt.subplots(1,5,figsize=(15,5))\nfor i,col in enumerate(num_cols_list):\n    sns.kdeplot(data=df_p[col],ax=axs[i],fill=True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:45.843663Z","iopub.execute_input":"2025-02-14T23:31:45.844065Z","iopub.status.idle":"2025-02-14T23:31:46.972741Z","shell.execute_reply.started":"2025-02-14T23:31:45.844037Z","shell.execute_reply":"2025-02-14T23:31:46.971444Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Nuemric variables box plot to cehck for outliers\ndf_p.boxplot(column=num_cols_list)","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:50.554968Z","iopub.execute_input":"2025-02-14T23:31:50.555594Z","iopub.status.idle":"2025-02-14T23:31:50.816965Z","shell.execute_reply.started":"2025-02-14T23:31:50.555509Z","shell.execute_reply":"2025-02-14T23:31:50.815675Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_p.boxplot(column=['LoanAmount','Loan_Amount_Term'])\n# It can be seen from the below plot that the range of these numeric variables is wide and there seem to be outliers for loan_amount_term","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:53.804102Z","iopub.execute_input":"2025-02-14T23:31:53.804505Z","iopub.status.idle":"2025-02-14T23:31:54.12285Z","shell.execute_reply.started":"2025-02-14T23:31:53.804474Z","shell.execute_reply":"2025-02-14T23:31:54.121253Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_p.boxplot(column=['ApplicantIncome','CoapplicantIncome'])\n# There seem to be instances where coapplicant income is higher than applicant income","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:31:57.643796Z","iopub.execute_input":"2025-02-14T23:31:57.644348Z","iopub.status.idle":"2025-02-14T23:31:57.830261Z","shell.execute_reply.started":"2025-02-14T23:31:57.644301Z","shell.execute_reply":"2025-02-14T23:31:57.829144Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_cols_list","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:32:00.803707Z","iopub.execute_input":"2025-02-14T23:32:00.804118Z","iopub.status.idle":"2025-02-14T23:32:00.813127Z","shell.execute_reply.started":"2025-02-14T23:32:00.804089Z","shell.execute_reply":"2025-02-14T23:32:00.811086Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Categorical variables distribution plot","metadata":{}},{"cell_type":"code","source":"# Catgeorical variables counts plot\nfig,axes=plt.subplots(nrows=1,ncols=len(cat_cols_list)-1,figsize=(15,5))\nfor i,col in enumerate(cat_cols_list[1:]):\n    cat_counts =df_p[col].value_counts()\n    cat_counts.plot(kind='bar',ax=axes[i])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:32:04.501041Z","iopub.execute_input":"2025-02-14T23:32:04.502051Z","iopub.status.idle":"2025-02-14T23:32:05.439109Z","shell.execute_reply.started":"2025-02-14T23:32:04.502002Z","shell.execute_reply":"2025-02-14T23:32:05.437785Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predictors relationship with target","metadata":{}},{"cell_type":"code","source":"#Distributions \nfig,axes = plt.subplots(nrows=1,ncols=len(num_cols_list),figsize=(15,5))\nfor i,col in enumerate(num_cols_list):\n    sns.histplot(data=df_p,ax=axes[i],x=col,kde=True,hue='Loan_Status')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:34:29.453998Z","iopub.execute_input":"2025-02-14T23:34:29.454436Z","iopub.status.idle":"2025-02-14T23:34:31.306004Z","shell.execute_reply.started":"2025-02-14T23:34:29.454409Z","shell.execute_reply":"2025-02-14T23:34:31.304795Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,axes =plt.subplots(nrows=1,ncols=len(num_cols_list),figsize=(15,5))\nfor i,col in enumerate(num_cols_list):\n    sns.stripplot(df_p,x='Loan_Status',y=col,ax=axes[i])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:34:36.47246Z","iopub.execute_input":"2025-02-14T23:34:36.472912Z","iopub.status.idle":"2025-02-14T23:34:37.473928Z","shell.execute_reply.started":"2025-02-14T23:34:36.47288Z","shell.execute_reply":"2025-02-14T23:34:37.472598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df_p.Loan_Status=df_p.Loan_Status.map(dict(Y=1,N=0))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T21:57:07.541259Z","iopub.execute_input":"2024-07-13T21:57:07.541652Z","iopub.status.idle":"2024-07-13T21:57:07.546845Z","shell.execute_reply.started":"2024-07-13T21:57:07.541621Z","shell.execute_reply":"2024-07-13T21:57:07.545557Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=len(cat_cols_list)-1,figsize=(15,5))\nfor i,col in enumerate(cat_cols_list):\n    if col not in ['Loan_Status','Loan_ID']:\n        sns.countplot(df_p,ax=axes[i],x='Loan_Status',hue=col)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-14T23:34:42.365558Z","iopub.execute_input":"2025-02-14T23:34:42.365929Z","iopub.status.idle":"2025-02-14T23:34:43.554055Z","shell.execute_reply.started":"2025-02-14T23:34:42.365903Z","shell.execute_reply":"2025-02-14T23:34:43.552561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing\n\nThe objective is to have a pipeline with the preprocessing and model building steps that can be used on any similar dataset# Steps\n\n1. Convert all numeric to double and categorical to string\n2. Impute missing values - numerical with median and categorical with 'unknown'\n3. String indexer to make give numeric labels to categorical variables\n4. One hot encoding before applying logistic regression model","metadata":{}},{"cell_type":"code","source":"df.printSchema()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T02:39:19.604582Z","iopub.execute_input":"2025-02-16T02:39:19.60494Z","iopub.status.idle":"2025-02-16T02:39:19.614303Z","shell.execute_reply.started":"2025-02-16T02:39:19.604915Z","shell.execute_reply":"2025-02-16T02:39:19.612952Z"}},"outputs":[{"name":"stdout","text":"root\n |-- Loan_ID: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Married: string (nullable = true)\n |-- Dependents: string (nullable = true)\n |-- Education: string (nullable = true)\n |-- Self_Employed: string (nullable = true)\n |-- ApplicantIncome: integer (nullable = true)\n |-- CoapplicantIncome: double (nullable = true)\n |-- LoanAmount: double (nullable = true)\n |-- Loan_Amount_Term: double (nullable = true)\n |-- Credit_History: double (nullable = true)\n |-- Property_Area: string (nullable = true)\n |-- Loan_Status: string (nullable = true)\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from pyspark.ml import Estimator, Transformer, Pipeline\nfrom pyspark.ml.param import Param, Params, TypeConverters\nfrom pyspark.ml.param.shared import HasInputCols, HasOutputCols\nfrom pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\nfrom pyspark.sql.functions import col, median, when,lit\nfrom pyspark.sql.types import DoubleType, StringType\nfrom pyspark.ml.feature import Imputer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:06:10.382306Z","iopub.execute_input":"2025-02-16T03:06:10.382978Z","iopub.status.idle":"2025-02-16T03:06:10.515083Z","shell.execute_reply.started":"2025-02-16T03:06:10.382926Z","shell.execute_reply":"2025-02-16T03:06:10.514129Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:06:17.359819Z","iopub.execute_input":"2025-02-16T03:06:17.36017Z","iopub.status.idle":"2025-02-16T03:06:17.364297Z","shell.execute_reply.started":"2025-02-16T03:06:17.360145Z","shell.execute_reply":"2025-02-16T03:06:17.363432Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"#### Custom Estimator and Transformer to change datatypes of specified columns into numeric and string datatypes","metadata":{}},{"cell_type":"markdown","source":"\n* Estimator: This is the main class that DataTypeConverter inherits from, providing the basic structure and functionality of an estimator in the PySpark ML pipeline.\n* DefaultParamsReadable: This mixin class provides functionality for reading parameters from a saved model. It allows the DataTypeConverter to be loaded from disk after being saved as part of a pipeline.\n* DefaultParamsWritable: This mixin class provides functionality for writing parameters when saving a model. It allows the DataTypeConverter to be saved to disk as part of a pipeline.\n* Param: Defines parameters for ML components, specifying name, description, and type; ensures type safety and provides documentation. _setDefault() & Getters/Setters: Set default parameter values and provide controlled access, ensuring seamless integration with PySpark's ML Pipelines.\n","metadata":{}},{"cell_type":"code","source":"class DataTypeConverter(Estimator, DefaultParamsReadable, DefaultParamsWritable):\n    \n    numericCols = Param(Params._dummy(), \"numericCols\", \"Columns to be converted to DoubleType\",\n                        typeConverter=TypeConverters.toListString)\n    categoricalCols = Param(Params._dummy(), \"categoricalCols\", \"Columns to be converted to StringType\",\n                            typeConverter=TypeConverters.toListString)\n\n    def __init__(self, numericCols=None, categoricalCols=None):\n        super(DataTypeConverter, self).__init__()\n        self._setDefault(numericCols=[], categoricalCols=[])\n        self.setNumericCols(numericCols)\n        self.setCategoricalCols(categoricalCols)\n\n    def setNumericCols(self, value):\n        return self._set(numericCols=value)\n\n    def getNumericCols(self):\n        return self.getOrDefault(self.numericCols)\n\n    def setCategoricalCols(self, value):\n        return self._set(categoricalCols=value)\n\n    def getCategoricalCols(self):\n        return self.getOrDefault(self.categoricalCols)\n        \n    def _fit(self, dataset):\n        return DataTypeConverterModel(self.getNumericCols(), self.getCategoricalCols())\n\nclass DataTypeConverterModel(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n    numericCols = Param(Params._dummy(), \"numericCols\", \"Columns to be converted to DoubleType\",\n                        typeConverter=TypeConverters.toListString)\n    categoricalCols = Param(Params._dummy(), \"categoricalCols\", \"Columns to be converted to StringType\",\n                            typeConverter=TypeConverters.toListString)\n    \n    def __init__(self, numericCols=None, categoricalCols=None):\n        super(DataTypeConverterModel, self).__init__()\n        self._setDefault(numericCols=[], categoricalCols=[])\n        self.setNumericCols(numericCols)\n        self.setCategoricalCols(categoricalCols)\n\n    def setNumericCols(self, value):\n        return self._set(numericCols=value)\n\n    def getNumericCols(self):\n        return self.getOrDefault(self.numericCols)\n\n    def setCategoricalCols(self, value):\n        return self._set(categoricalCols=value)\n\n    def getCategoricalCols(self):\n        return self.getOrDefault(self.categoricalCols)\n        \n    def _transform(self,dataset):\n        for col_name in self.getNumericCols():\n            dataset = dataset.withColumn(col_name,col(col_name).cast(DoubleType()))\n        for col_name in self.getCategoricalCols():\n            dataset = dataset.withColumn(col_name,col(col_name).cast(StringType()))\n        return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:06:24.917887Z","iopub.execute_input":"2025-02-16T03:06:24.918519Z","iopub.status.idle":"2025-02-16T03:06:24.930272Z","shell.execute_reply.started":"2025-02-16T03:06:24.918489Z","shell.execute_reply":"2025-02-16T03:06:24.929323Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class CustomImputer(Estimator, HasInputCols, HasOutputCols,\n                    DefaultParamsReadable, DefaultParamsWritable):\n    \n    inputCols = Param(Params._dummy(), \"inputCols\", \"Columns to be checked and imputed\",\n                        typeConverter=TypeConverters.toListString)\n\n        \n    def __init__(self, inputCols=None):\n        super(CustomImputer, self).__init__()\n        self._setDefault(inputCols=[])\n        self.setInputCols(inputCols)\n\n    def setInputCols(self, value):\n        return self._set(inputCols=value)\n\n    def getInputCols(self):\n        return self.getOrDefault(self.inputCols)\n        \n    def _fit(self, dataset):\n        \"get numeric cols from inputcols list and get the fill value as median aka numericaimputer\"\n        \"get categorical cols from inputcols list and get the fill value aka categoricalimputer\"\n        numericCols = [field.name for field in dataset.schema.fields if isinstance(field.dataType, DoubleType)]\n        categoricalCols = [field.name for field in dataset.schema.fields if isinstance(field.dataType, StringType)]\n        # Initialize an empty dictionary to store the medians\n        median_values = {}\n\n        # Iterate through the numeric columns and calculate median\n        for column in numericCols:\n        # Calculate the median\n            median_value = df.select(median(col(column))).collect()[0][0]\n    \n        # Store the median in the dictionary\n            median_values[column] = median_value\n        return CustomImputerModel(inputCols=self.getInputCols(), \n                                  numericCols=numericCols,\n                                  categoricalCols=categoricalCols,\n                                  numericFill=median_values,\n                                 categoricalFill='unknown')\n\nclass CustomImputerModel(Transformer, HasInputCols, HasOutputCols):\n    inputCols = Param(Params._dummy(),\"inputCols\",\"InputColumns to impute\",\n                     typeConverter= TypeConverters.toListString)\n    numericCols = Param(Params._dummy(), \"numericCols\", \"Numeric columns to impute\",\n                        typeConverter=TypeConverters.toListString)\n    categoricalCols = Param(Params._dummy(), \"categoricalCols\", \"Categorical columns to impute\",\n                            typeConverter=TypeConverters.toListString)\n    numericFill = Param(Params._dummy(), \"numericFill\", \"JSON string of numeric fill values\")\n    categoricalFill = Param(Params._dummy(), \"categoricalFill\", \"Fill value for categorical columns\",\n                            typeConverter=TypeConverters.toString)\n    \n    def __init__(self, inputCols=None, numericCols=None, categoricalCols=None, numericFill=None, categoricalFill=None):\n        super(CustomImputerModel, self).__init__()\n        self._setDefault(inputCols=[],numericCols=[], categoricalCols=[], numericFill={}, categoricalFill=\"unknown\")\n        self.setInputCols(inputCols)\n        self.setNumericCols(numericCols)\n        self.setCategoricalCols(categoricalCols)\n        self.setNumericFill(numericFill)\n        self.setCategoricalFill(categoricalFill)\n\n    def setInputCols(self,value):\n        return self._set(inputCols=value)\n\n    def setNumericCols(self, value):\n        return self._set(numericCols=value)\n\n    def getNumericCols(self):\n        return self.getOrDefault(self.numericCols)\n\n    def setCategoricalCols(self, value):\n        return self._set(categoricalCols=value)\n\n    def getCategoricalCols(self):\n        return self.getOrDefault(self.categoricalCols)\n\n    def setNumericFill(self, value):\n        return self._set(numericFill=json.dumps(value))\n\n    def getNumericFill(self):\n        return json.loads(self.getOrDefault(self.numericFill))\n\n    def setCategoricalFill(self, value):\n        return self._set(categoricalFill=value)\n\n    def getCategoricalFill(self):\n        return self.getOrDefault(self.categoricalFill)\n\n   \n    def _transform(self, dataset):\n        \n        numericFill = self.getNumericFill()\n        for column in self.getNumericCols():\n            \n            if column in self.getInputCols():\n                dataset = dataset.withColumn(column, when(col(column).isNull(), lit(numericFill[column])).otherwise(col(column)))\n        \n        # Impute categorical columns\n        for column in self.getCategoricalCols():\n            if column in self.getInputCols():\n                dataset = dataset.withColumn(column, when(col(column).isNull(), lit(self.getCategoricalFill())).otherwise(col(column)))\n        \n        return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:06:38.667094Z","iopub.execute_input":"2025-02-16T03:06:38.667476Z","iopub.status.idle":"2025-02-16T03:06:38.685058Z","shell.execute_reply.started":"2025-02-16T03:06:38.667449Z","shell.execute_reply":"2025-02-16T03:06:38.684108Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df.toPandas().head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T23:36:26.769109Z","iopub.execute_input":"2025-02-14T23:36:26.769498Z","iopub.status.idle":"2025-02-14T23:36:26.87915Z","shell.execute_reply.started":"2025-02-14T23:36:26.769471Z","shell.execute_reply":"2025-02-14T23:36:26.877724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_type_converter = DataTypeConverter(numericCols=['ApplicantIncome','CoapplicantIncome',\n                                                    'Loanamount','Loan_Amount_Term'], \n                                        categoricalCols=['Gender', 'Married','Dependents','Education','Self_Employed',\n                                                       'Property_Area','Loan_Status' ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:06:45.981709Z","iopub.execute_input":"2025-02-16T03:06:45.982585Z","iopub.status.idle":"2025-02-16T03:06:45.987447Z","shell.execute_reply.started":"2025-02-16T03:06:45.982553Z","shell.execute_reply":"2025-02-16T03:06:45.986437Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"custom_imputer = CustomImputer(inputCols=['ApplicantIncome','CoapplicantIncome',\n                                        'LoanAmount','Loan_Amount_Term', \n                                        'Gender', 'Married','Dependents','Education',\n                                          'Self_Employed','Credit_History',\n                                            'Property_Area','Loan_Status' ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:06:48.901162Z","iopub.execute_input":"2025-02-16T03:06:48.902029Z","iopub.status.idle":"2025-02-16T03:06:48.906693Z","shell.execute_reply.started":"2025-02-16T03:06:48.901996Z","shell.execute_reply":"2025-02-16T03:06:48.905641Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"pipeline=Pipeline(stages=[data_type_converter,custom_imputer])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:07:17.461852Z","iopub.execute_input":"2025-02-16T03:07:17.462206Z","iopub.status.idle":"2025-02-16T03:07:17.46713Z","shell.execute_reply.started":"2025-02-16T03:07:17.462177Z","shell.execute_reply":"2025-02-16T03:07:17.466075Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"preprocessed_pipeline=pipeline.fit(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:07:20.602527Z","iopub.execute_input":"2025-02-16T03:07:20.603515Z","iopub.status.idle":"2025-02-16T03:07:22.630099Z","shell.execute_reply.started":"2025-02-16T03:07:20.603484Z","shell.execute_reply":"2025-02-16T03:07:22.62907Z"}},"outputs":[{"name":"stdout","text":"['ApplicantIncome', 'CoapplicantIncome', 'Loanamount', 'Loan_Amount_Term', 'Credit_History']\n['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"preprocessed_df=preprocessed_pipeline.transform(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:07:27.085954Z","iopub.execute_input":"2025-02-16T03:07:27.086776Z","iopub.status.idle":"2025-02-16T03:07:27.686114Z","shell.execute_reply.started":"2025-02-16T03:07:27.086743Z","shell.execute_reply":"2025-02-16T03:07:27.685131Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:07:31.118376Z","iopub.execute_input":"2025-02-16T03:07:31.1191Z","iopub.status.idle":"2025-02-16T03:07:31.127273Z","shell.execute_reply.started":"2025-02-16T03:07:31.119072Z","shell.execute_reply":"2025-02-16T03:07:31.126264Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[('Loan_ID', 'string'),\n ('Gender', 'string'),\n ('Married', 'string'),\n ('Dependents', 'string'),\n ('Education', 'string'),\n ('Self_Employed', 'string'),\n ('ApplicantIncome', 'int'),\n ('CoapplicantIncome', 'double'),\n ('LoanAmount', 'double'),\n ('Loan_Amount_Term', 'double'),\n ('Credit_History', 'double'),\n ('Property_Area', 'string'),\n ('Loan_Status', 'string')]"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"preprocessed_df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:07:35.637565Z","iopub.execute_input":"2025-02-16T03:07:35.637912Z","iopub.status.idle":"2025-02-16T03:07:35.646459Z","shell.execute_reply.started":"2025-02-16T03:07:35.637886Z","shell.execute_reply":"2025-02-16T03:07:35.645558Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[('Loan_ID', 'string'),\n ('Gender', 'string'),\n ('Married', 'string'),\n ('Dependents', 'string'),\n ('Education', 'string'),\n ('Self_Employed', 'string'),\n ('ApplicantIncome', 'double'),\n ('CoapplicantIncome', 'double'),\n ('Loanamount', 'double'),\n ('Loan_Amount_Term', 'double'),\n ('Credit_History', 'double'),\n ('Property_Area', 'string'),\n ('Loan_Status', 'string')]"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"df.toPandas().isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:09:43.24633Z","iopub.execute_input":"2025-02-16T03:09:43.246681Z","iopub.status.idle":"2025-02-16T03:09:43.385226Z","shell.execute_reply.started":"2025-02-16T03:09:43.246658Z","shell.execute_reply":"2025-02-16T03:09:43.384267Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Loan_ID               0\nGender                5\nMarried               0\nDependents            8\nEducation             0\nSelf_Employed        21\nApplicantIncome       0\nCoapplicantIncome     0\nLoanAmount            0\nLoan_Amount_Term     11\nCredit_History       30\nProperty_Area         0\nLoan_Status           0\ndtype: int64"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"preprocessed_df.toPandas().isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T03:08:58.171263Z","iopub.execute_input":"2025-02-16T03:08:58.171923Z","iopub.status.idle":"2025-02-16T03:08:58.534655Z","shell.execute_reply.started":"2025-02-16T03:08:58.171894Z","shell.execute_reply":"2025-02-16T03:08:58.533703Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Loan_ID              0\nGender               0\nMarried              0\nDependents           0\nEducation            0\nSelf_Employed        0\nApplicantIncome      0\nCoapplicantIncome    0\nLoanamount           0\nLoan_Amount_Term     0\nCredit_History       0\nProperty_Area        0\nLoan_Status          0\ndtype: int64"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"numericCols = [field.name for field in df.schema.fields if isinstance(field.dataType, DoubleType)]\ncategoricalCols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n        # Initialize an empty dictionary to store the medians","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T00:55:59.639175Z","iopub.execute_input":"2025-02-15T00:55:59.639639Z","iopub.status.idle":"2025-02-15T00:55:59.645638Z","shell.execute_reply.started":"2025-02-15T00:55:59.639606Z","shell.execute_reply":"2025-02-15T00:55:59.644504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numericCols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T00:56:06.739248Z","iopub.execute_input":"2025-02-15T00:56:06.739696Z","iopub.status.idle":"2025-02-15T00:56:06.746755Z","shell.execute_reply.started":"2025-02-15T00:56:06.739665Z","shell.execute_reply":"2025-02-15T00:56:06.745611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categoricalCols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T00:56:15.983976Z","iopub.execute_input":"2025-02-15T00:56:15.98436Z","iopub.status.idle":"2025-02-15T00:56:15.992467Z","shell.execute_reply.started":"2025-02-15T00:56:15.984333Z","shell.execute_reply":"2025-02-15T00:56:15.991054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomImputer(Estimator, HasInputCols, HasOutputCols,\n                    DefaultParamsReadable, DefaultParamsWritable):\n    \n    inputCols = Param(Params._dummy(), \"inputCols\", \"Columns to be checked and imputed\",\n                        typeConverter=TypeConverters.toListString)\n\n        \n    def __init__(self, inputCols=None):\n        super(CustomImputer, self).__init__()\n        self._setDefault(inputCols=[])\n        self.setInputCols(inputCols)\n\n    def setInputCols(self, value):\n        return self._set(inputCols=value)\n\n    def getInputCols(self):\n        return self.getOrDefault(self.inputCols)\n        \n    def _fit(self, dataset):\n        \"get numeric cols from inputcols list and get the fill value as median aka numericaimputer\"\n        \"get categorical cols from inputcols list and get the fill value aka categoricalimputer\"\n        numericCols = [field.name for field in dataset.schema.fields if isinstance(field.dataType, DoubleType)]\n        categoricalCols = [field.name for field in dataset.schema.fields if isinstance(field.dataType, StringType)]\n        print(numericCols)\n        print(categoricalCols)\n        # Initialize an empty dictionary to store the medians\n        median_values = {}\n\n        # Iterate through the numeric columns and calculate median\n        for column in numericCols:\n        # Calculate the median\n            median_value = df.select(median(col(column))).collect()[0][0]\n    \n        # Store the median in the dictionary\n            median_values[column] = median_value\n        return CustomImputerModel(inputCols=self.getInputCols(), \n                                  numericCols=numericCols,\n                                  categoricalCols=categoricalCols,\n                                  numericFill=median_values,\n                                 categoricalFill='unknown')\n\nclass CustomImputerModel(Transformer, HasInputCols, HasOutputCols):\n    inputCols = Param(Params._dummy(),\"inputCols\",\"InputColumns to impute\",\n                     typeConverter= TypeConverters.toListString)\n    numericCols = Param(Params._dummy(), \"numericCols\", \"Numeric columns to impute\",\n                        typeConverter=TypeConverters.toListString)\n    categoricalCols = Param(Params._dummy(), \"categoricalCols\", \"Categorical columns to impute\",\n                            typeConverter=TypeConverters.toListString)\n    numericFill = Param(Params._dummy(), \"numericFill\", \"JSON string of numeric fill values\")\n    categoricalFill = Param(Params._dummy(), \"categoricalFill\", \"Fill value for categorical columns\",\n                            typeConverter=TypeConverters.toString)\n    \n    def __init__(self, inputCols=None, numericCols=None, categoricalCols=None, numericFill=None, categoricalFill=None):\n        super(CustomImputerModel, self).__init__()\n        self._setDefault(inputCols=[],numericCols=[], categoricalCols=[], numericFill={}, categoricalFill=\"unknown\")\n        self.setInputCols(inputCols)\n        self.setNumericCols(numericCols)\n        self.setCategoricalCols(categoricalCols)\n        self.setNumericFill(numericFill)\n        self.setCategoricalFill(categoricalFill)\n\n    def setInputCols(self,value):\n        return self._set(inputCols=value)\n\n    def setNumericCols(self, value):\n        return self._set(numericCols=value)\n\n    def getNumericCols(self):\n        return self.getOrDefault(self.numericCols)\n\n    def setCategoricalCols(self, value):\n        return self._set(categoricalCols=value)\n\n    def getCategoricalCols(self):\n        return self.getOrDefault(self.categoricalCols)\n\n    def setNumericFill(self, value):\n        return self._set(numericFill=json.dumps(value))\n\n    def getNumericFill(self):\n        return json.loads(self.getOrDefault(self.numericFill))\n\n    def setCategoricalFill(self, value):\n        return self._set(categoricalFill=value)\n\n    def getCategoricalFill(self):\n        return self.getOrDefault(self.categoricalFill)\n\n   \n    def _transform(self, dataset):\n        \n        numericFill = self.getNumericFill()\n        print(numericFill)\n        print(self.getNumericCols())\n        for column in self.getNumericCols():\n            \n            if column in self.getInputCols():\n                print(column)\n                dataset = dataset.withColumn(column, when(col(column).isNull(), lit(numericFill[column])).otherwise(col(column)))\n        \n        # Impute categorical columns\n        for column in self.getCategoricalCols():\n            if column in self.getInputCols():\n                dataset = dataset.withColumn(column, when(col(column).isNull(), lit(self.getCategoricalFill())).otherwise(col(column)))\n        \n        return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:21:14.756966Z","iopub.execute_input":"2025-02-15T01:21:14.757383Z","iopub.status.idle":"2025-02-15T01:21:14.777884Z","shell.execute_reply.started":"2025-02-15T01:21:14.757356Z","shell.execute_reply":"2025-02-15T01:21:14.776495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.toPandas().isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T00:36:05.93466Z","iopub.execute_input":"2025-02-15T00:36:05.935091Z","iopub.status.idle":"2025-02-15T00:36:06.147858Z","shell.execute_reply.started":"2025-02-15T00:36:05.935062Z","shell.execute_reply":"2025-02-15T00:36:06.146629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_imputer = CustomImputer(inputCols=['ApplicantIncome','CoapplicantIncome',\n                                        'LoanAmount','Loan_Amount_Term', \n                                        'Gender', 'Married','Dependents','Education',\n                                          'Self_Employed','Credit_History',\n                                            'Property_Area','Loan_Status' ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:21:19.244789Z","iopub.execute_input":"2025-02-15T01:21:19.245155Z","iopub.status.idle":"2025-02-15T01:21:19.251304Z","shell.execute_reply.started":"2025-02-15T01:21:19.245128Z","shell.execute_reply":"2025-02-15T01:21:19.249997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline=Pipeline(stages=[custom_imputer])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:21:22.312694Z","iopub.execute_input":"2025-02-15T01:21:22.313138Z","iopub.status.idle":"2025-02-15T01:21:22.319119Z","shell.execute_reply.started":"2025-02-15T01:21:22.313107Z","shell.execute_reply":"2025-02-15T01:21:22.317823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessed_pipeline=pipeline.fit(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:21:24.752322Z","iopub.execute_input":"2025-02-15T01:21:24.752731Z","iopub.status.idle":"2025-02-15T01:21:25.335281Z","shell.execute_reply.started":"2025-02-15T01:21:24.752701Z","shell.execute_reply":"2025-02-15T01:21:25.334586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.toPandas().head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:01:01.888109Z","iopub.execute_input":"2025-02-15T01:01:01.888635Z","iopub.status.idle":"2025-02-15T01:01:02.012051Z","shell.execute_reply.started":"2025-02-15T01:01:01.888506Z","shell.execute_reply":"2025-02-15T01:01:02.010621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T00:58:20.687907Z","iopub.execute_input":"2025-02-15T00:58:20.688348Z","iopub.status.idle":"2025-02-15T00:58:20.696691Z","shell.execute_reply.started":"2025-02-15T00:58:20.688318Z","shell.execute_reply":"2025-02-15T00:58:20.695454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformed_df=preprocessed_pipeline.transform(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:21:31.48162Z","iopub.execute_input":"2025-02-15T01:21:31.482029Z","iopub.status.idle":"2025-02-15T01:21:31.659838Z","shell.execute_reply.started":"2025-02-15T01:21:31.482001Z","shell.execute_reply":"2025-02-15T01:21:31.658599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##Archive","metadata":{}},{"cell_type":"code","source":"transformed_df.toPandas().isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:22:30.133178Z","iopub.execute_input":"2025-02-15T01:22:30.133604Z","iopub.status.idle":"2025-02-15T01:22:30.337383Z","shell.execute_reply.started":"2025-02-15T01:22:30.133556Z","shell.execute_reply":"2025-02-15T01:22:30.336211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(df)","metadata":{"execution":{"iopub.status.busy":"2025-02-15T00:35:08.617912Z","iopub.execute_input":"2025-02-15T00:35:08.618303Z","iopub.status.idle":"2025-02-15T00:35:08.62544Z","shell.execute_reply.started":"2025-02-15T00:35:08.618277Z","shell.execute_reply":"2025-02-15T00:35:08.624304Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-08-02T03:25:56.249869Z","iopub.execute_input":"2024-08-02T03:25:56.251432Z","iopub.status.idle":"2024-08-02T03:25:56.261561Z","shell.execute_reply.started":"2024-08-02T03:25:56.251365Z","shell.execute_reply":"2024-08-02T03:25:56.259998Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.toPandas().head()","metadata":{"execution":{"iopub.status.busy":"2024-08-02T03:26:32.592992Z","iopub.execute_input":"2024-08-02T03:26:32.593521Z","iopub.status.idle":"2024-08-02T03:26:32.723594Z","shell.execute_reply.started":"2024-08-02T03:26:32.593481Z","shell.execute_reply":"2024-08-02T03:26:32.722257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.toPandas().head()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:01:35.620032Z","iopub.execute_input":"2024-08-03T16:01:35.620522Z","iopub.status.idle":"2024-08-03T16:01:36.328639Z","shell.execute_reply.started":"2024-08-03T16:01:35.620478Z","shell.execute_reply":"2024-08-03T16:01:36.327225Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:16:08.923889Z","iopub.execute_input":"2024-08-03T16:16:08.92439Z","iopub.status.idle":"2024-08-03T16:16:08.936336Z","shell.execute_reply.started":"2024-08-03T16:16:08.924347Z","shell.execute_reply":"2024-08-03T16:16:08.934858Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#imposing double to all numeric and string to all string fields\nprint(num_cols_list,'\\n',cat_cols_list,'\\n',oth_cols_list)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:17:18.650763Z","iopub.execute_input":"2024-08-03T16:17:18.651275Z","iopub.status.idle":"2024-08-03T16:17:18.659852Z","shell.execute_reply.started":"2024-08-03T16:17:18.651235Z","shell.execute_reply":"2024-08-03T16:17:18.658377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's impute missing values","metadata":{}},{"cell_type":"code","source":"for col in num_cols_list:\n    df = df.withColumn(col, df[col].cast(DoubleType()))","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:19:18.62872Z","iopub.execute_input":"2024-08-03T16:19:18.62916Z","iopub.status.idle":"2024-08-03T16:19:18.831994Z","shell.execute_reply.started":"2024-08-03T16:19:18.629126Z","shell.execute_reply":"2024-08-03T16:19:18.830342Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:19:28.45654Z","iopub.execute_input":"2024-08-03T16:19:28.457634Z","iopub.status.idle":"2024-08-03T16:19:28.469795Z","shell.execute_reply.started":"2024-08-03T16:19:28.457587Z","shell.execute_reply":"2024-08-03T16:19:28.468403Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in cat_cols_list:\n    df = df.withColumn(col, df[col].cast(StringType()))","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:19:53.9431Z","iopub.execute_input":"2024-08-03T16:19:53.943671Z","iopub.status.idle":"2024-08-03T16:19:54.223604Z","shell.execute_reply.started":"2024-08-03T16:19:53.943631Z","shell.execute_reply":"2024-08-03T16:19:54.221794Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:20:00.182443Z","iopub.execute_input":"2024-08-03T16:20:00.182869Z","iopub.status.idle":"2024-08-03T16:20:00.19065Z","shell.execute_reply.started":"2024-08-03T16:20:00.182836Z","shell.execute_reply":"2024-08-03T16:20:00.189483Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first, load the dataframe\ncheck the fields, their datatypes and missing values\nnext, convert all numeric to double types\nand categorical to string, even though it has numeric values (as in 1,2,3- but it's category)\nnext, impute missing values\nnext,do one hot encoding on categorical variables\n                                                              ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.toPandas().head(2)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:20:27.366651Z","iopub.execute_input":"2024-08-03T16:20:27.367073Z","iopub.status.idle":"2024-08-03T16:20:27.849243Z","shell.execute_reply.started":"2024-08-03T16:20:27.36704Z","shell.execute_reply":"2024-08-03T16:20:27.847808Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.feature import Imputer","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:03:11.201815Z","iopub.execute_input":"2024-08-03T16:03:11.202239Z","iopub.status.idle":"2024-08-03T16:03:11.209227Z","shell.execute_reply.started":"2024-08-03T16:03:11.202208Z","shell.execute_reply":"2024-08-03T16:03:11.207377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Imputer(inputCols=[\"value\"], outputCols=[\"value_imputed\"]).setStrategy(\"median\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imputer = Imputer(inputCols=num_cols_list, outputCols=num_cols_list).setStrategy(\"median\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:25:16.725911Z","iopub.execute_input":"2024-08-03T16:25:16.726428Z","iopub.status.idle":"2024-08-03T16:25:16.740138Z","shell.execute_reply.started":"2024-08-03T16:25:16.726393Z","shell.execute_reply":"2024-08-03T16:25:16.738708Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_imputed = imputer.fit(df).transform(df)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:25:18.846668Z","iopub.execute_input":"2024-08-03T16:25:18.847124Z","iopub.status.idle":"2024-08-03T16:25:20.034618Z","shell.execute_reply.started":"2024-08-03T16:25:18.847088Z","shell.execute_reply":"2024-08-03T16:25:20.033445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.toPandas().isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:25:58.924682Z","iopub.execute_input":"2024-08-03T16:25:58.926159Z","iopub.status.idle":"2024-08-03T16:25:59.095408Z","shell.execute_reply.started":"2024-08-03T16:25:58.926116Z","shell.execute_reply":"2024-08-03T16:25:59.094182Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_imputed.toPandas().isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T16:25:44.349087Z","iopub.execute_input":"2024-08-03T16:25:44.349533Z","iopub.status.idle":"2024-08-03T16:25:44.74268Z","shell.execute_reply.started":"2024-08-03T16:25:44.349499Z","shell.execute_reply":"2024-08-03T16:25:44.741472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Imputing categorical feaures, the imputer doesnt work, hecne creating custom transformer","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline, Transformer, Estimator\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params\nfrom pyspark.sql.functions import col, lit\nfrom pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable","metadata":{"execution":{"iopub.status.busy":"2024-08-03T22:27:31.071298Z","iopub.execute_input":"2024-08-03T22:27:31.072287Z","iopub.status.idle":"2024-08-03T22:27:31.077559Z","shell.execute_reply.started":"2024-08-03T22:27:31.072252Z","shell.execute_reply":"2024-08-03T22:27:31.076338Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml import Estimator, Model, Transformer\nfrom pyspark.ml.param.shared import HasInputCols, HasOutputCols, Param, Params\nfrom pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\nfrom pyspark.sql.functions import col\n\nclass CustomImputerEstimator(Estimator, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n    fillValue = Param(Params._dummy(), \"fillValue\", \"Value to replace missing values with\")\n\n    def __init__(self, inputCols=None, outputCols=None, fillValue=\"missing\"):\n        super(CustomImputerEstimator, self).__init__()\n        self._setDefault(fillValue=\"missing\")\n        self.setParams(inputCols=inputCols, outputCols=outputCols, fillValue=fillValue)\n    \n    def setParams(self, inputCols=None, outputCols=None, fillValue=\"missing\"):\n        if inputCols is not None:\n            self.setInputCols(inputCols)\n        if outputCols is not None:\n            self.setOutputCols(outputCols)\n        if fillValue is not None:\n            self.setFillValue(fillValue)\n        return self\n    \n    def getFillValue(self):\n        return self.getOrDefault(self.fillValue)\n\n    def setFillValue(self, value):\n        return self._set(fillValue=value)\n    \n    def setInputCols(self, value):\n        return self._set(inputCols=value)\n    \n    def setOutputCols(self, value):\n        return self._set(outputCols=value)\n    \n    def _fit(self, dataset):\n        return CustomImputerModel(inputCols=self.getInputCols(), \n                                  outputCols=self.getOutputCols(), \n                                  fillValue=self.getFillValue())\n\nclass CustomImputerModel(Model, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n    fillValue = Param(Params._dummy(), \"fillValue\", \"Value to replace missing values with\")\n\n    def __init__(self, inputCols=None, outputCols=None, fillValue=\"missing\"):\n        super(CustomImputerModel, self).__init__()\n        self._setDefault(fillValue=\"missing\")\n        self.setParams(inputCols=inputCols, outputCols=outputCols, fillValue=fillValue)\n    \n    def setParams(self, inputCols=None, outputCols=None, fillValue=\"missing\"):\n        if inputCols is not None:\n            self.setInputCols(inputCols)\n        if outputCols is not None:\n            self.setOutputCols(outputCols)\n        if fillValue is not None:\n            self.setFillValue(fillValue)\n        return self\n    \n    def getFillValue(self):\n        return self.getOrDefault(self.fillValue)\n\n    def setFillValue(self, value):\n        return self._set(fillValue=value)\n    \n    def setInputCols(self, value):\n        return self._set(inputCols=value)\n    \n    def setOutputCols(self, value):\n        return self._set(outputCols=value)\n    \n    def _transform(self, dataset):\n        inputCols = self.getInputCols()\n        outputCols = self.getOutputCols()\n        fillValue = self.getFillValue()\n        \n        for inputCol, outputCol in zip(inputCols, outputCols):\n            dataset = dataset.withColumn(outputCol, col(inputCol).cast(\"string\")).na.fill({outputCol: fillValue})\n        return dataset\n\n# Usage in a pipeline\nfrom pyspark.ml import Pipeline\n\n\n# Create the custom imputer estimator\nimputer = CustomImputerEstimator(inputCols=cat_cols_list, outputCols=cat_cols_list, fillValue=\"Unknown\")\n\n# Create a pipeline with the imputer\npipeline = Pipeline(stages=[imputer])\n\n# Fit the pipeline on the training data\nmodel = pipeline.fit(df_train)\n\n# Save the pipeline model\nmodel.save(\"path/to/save/pipeline_model\")\n\n# Later, load the model and transform the test data\nfrom pyspark.ml import PipelineModel\nloaded_model = PipelineModel.load(\"path/to/save/pipeline_model\")\nimputed_test_df = loaded_model.transform(df_test)\n\n# Show the result\nimputed_test_df.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Later, load the model and transform the test data\nfrom pyspark.ml import PipelineModel\nloaded_model = PipelineModel.load(\"path/to/save/pipeline_model\")\n\n# Create a sample test DataFrame\ndata_test = [(5, None), (6, \"b\"), (7, None)]\ndf_test = spark.createDataFrame(data_test, [\"id\", \"category\"])\n\n# Transform the test data\nimputed_test_df = loaded_model.transform(df_test)\n\n# Show the result\nimputed_test_df.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml import Transformer\nfrom pyspark.ml.param.shared import HasInputCols, HasOutputCols, Param, Params\nfrom pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\nfrom pyspark.sql.functions import col\n\nclass CustomImputer(Transformer, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n    fillValue = Param(Params._dummy(), \"fillValue\", \"Value to replace missing values with\")\n\n    def __init__(self, inputCols=None, outputCols=None, fillValue=\"missing\"):\n        super(CustomImputer, self).__init__()\n        self._setDefault(fillValue=\"missing\")\n        self.setParams(inputCols=inputCols, outputCols=outputCols, fillValue=fillValue)\n    \n    def setParams(self, inputCols=None, outputCols=None, fillValue=\"missing\"):\n        if inputCols is not None:\n            self.setInputCols(inputCols)\n        if outputCols is not None:\n            self.setOutputCols(outputCols)\n        if fillValue is not None:\n            self.setFillValue(fillValue)\n        return self\n    \n    def getFillValue(self):\n        return self.getOrDefault(self.fillValue)\n\n    def setFillValue(self, value):\n        return self._set(fillValue=value)\n    \n    def setInputCols(self, value):\n        return self._set(inputCols=value)\n    \n    def setOutputCols(self, value):\n        return self._set(outputCols=value)\n    \n    def _transform(self, dataset):\n        inputCols = self.getInputCols()\n        outputCols = self.getOutputCols()\n        fillValue = self.getFillValue()\n        \n        for inputCol, outputCol in zip(inputCols, outputCols):\n            dataset = dataset.withColumn(outputCol, col(inputCol).cast(\"string\")).na.fill({outputCol: fillValue})\n        return dataset\n\n\n# Instantiate the CustomImputer\nimputer_cat = CustomImputer(inputCols=cat_cols_list, outputCols=cat_cols_list, fillValue=\"Unknown\")\n\n# Apply the transformer\nimputed_cat_df = imputer_cat.transform(df)\n\n# Show the result\n#imputed_cat_df.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T22:50:32.041783Z","iopub.execute_input":"2024-08-03T22:50:32.042141Z","iopub.status.idle":"2024-08-03T22:50:32.35009Z","shell.execute_reply.started":"2024-08-03T22:50:32.042117Z","shell.execute_reply":"2024-08-03T22:50:32.348786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imputed_cat_df.toPandas().isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T22:50:46.853915Z","iopub.execute_input":"2024-08-03T22:50:46.854248Z","iopub.status.idle":"2024-08-03T22:50:47.495687Z","shell.execute_reply.started":"2024-08-03T22:50:46.854221Z","shell.execute_reply":"2024-08-03T22:50:47.494468Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.toPandas().isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T22:50:57.675953Z","iopub.execute_input":"2024-08-03T22:50:57.676997Z","iopub.status.idle":"2024-08-03T22:50:57.864431Z","shell.execute_reply.started":"2024-08-03T22:50:57.676955Z","shell.execute_reply":"2024-08-03T22:50:57.863355Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate the CustomImputer\nimputer_cat = CustomImputer(inputCol=cat_cols_list, outputCol=cat_cols_list, fillValue=\"Unknown\")\n\n# Apply the transformer\nimputed_cat_df = imputer_cat.transform(df)\n\n# Show the result\nimputed_cat_df.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T22:33:19.556211Z","iopub.execute_input":"2024-08-03T22:33:19.557202Z","iopub.status.idle":"2024-08-03T22:33:19.611185Z","shell.execute_reply.started":"2024-08-03T22:33:19.557167Z","shell.execute_reply":"2024-08-03T22:33:19.609853Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Archive","metadata":{}},{"cell_type":"code","source":"# Let's split the data into train, valid and test datatsets\ntrain,valid,test=df.randomSplit(weights=[0.7,0.2,0.1],seed=12345)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T21:57:09.65442Z","iopub.execute_input":"2024-07-13T21:57:09.655565Z","iopub.status.idle":"2024-07-13T21:57:09.70957Z","shell.execute_reply.started":"2024-07-13T21:57:09.65551Z","shell.execute_reply":"2024-07-13T21:57:09.708149Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Train dataset',train.count(),train.agg(F.sum('target')).collect()[0][0])\nprint('Valid dataset',valid.count(),valid.agg(F.sum('target')).collect()[0][0])\nprint('Test dataset',test.count(),test.agg(F.sum('target')).collect()[0][0])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T21:57:09.711747Z","iopub.execute_input":"2024-07-13T21:57:09.712211Z","iopub.status.idle":"2024-07-13T21:57:11.425152Z","shell.execute_reply.started":"2024-07-13T21:57:09.712172Z","shell.execute_reply":"2024-07-13T21:57:11.423642Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler","metadata":{"execution":{"iopub.status.busy":"2024-07-13T21:57:11.962052Z","iopub.status.idle":"2024-07-13T21:57:11.965558Z","shell.execute_reply.started":"2024-07-13T21:57:11.965157Z","shell.execute_reply":"2024-07-13T21:57:11.965197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.feature import StandardScaler,OneHotEncoder,StringIndexer, VectorAssembler, Imputer,ChiSqSelector","metadata":{"execution":{"iopub.status.busy":"2024-07-13T21:57:11.97413Z","iopub.status.idle":"2024-07-13T21:57:11.975184Z","shell.execute_reply.started":"2024-07-13T21:57:11.974878Z","shell.execute_reply":"2024-07-13T21:57:11.974908Z"},"trusted":true},"outputs":[],"execution_count":null}]}